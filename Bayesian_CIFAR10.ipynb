{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darkfapper/Pandoras_Box/blob/main/Bayesian_CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKNAoZxk3t78",
        "outputId": "325cfed7-aedb-4026-c8f8-3ad88a69c825"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 6s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import functools\n",
        "tfk = tf.keras\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "tfn = tfp.experimental.nn\n",
        "tfpl = tfp.layers\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "NUM_TRAIN_EXAMPLES = 100\n",
        "\n",
        "\n",
        "\n",
        "# Load in the data\n",
        "cifar10 = tf.keras.datasets.cifar10\n",
        "\n",
        "# Distribute it to train and test set\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train/ 255.\n",
        "x_test = x_test / 255.\n",
        "\n",
        "y_train_oh = to_categorical(y_train, NUM_CLASSES)\n",
        "y_test_oh = to_categorical(y_test, NUM_CLASSES)\n",
        "\n",
        "# x_train =np.expand_dims(x_train, -1).astype(\"float32\") / 255.\n",
        "# x_test =np.expand_dims(x_test, -1).astype(\"float32\") / 255.\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "epochs = 150\n",
        "num_monte_carlo = 50\n",
        "\n",
        "BayesConv2D = functools.partial(tfn.ConvolutionVariationalFlipout,penalty_weight=1. / x_train.shape[0],rank = 2)\n",
        "BayesDense = functools.partial(tfn.AffineVariationalFlipout,penalty_weight=1. / x_train.shape[0])\n",
        "\n",
        "tf.experimental.numpy.experimental_enable_numpy_behavior()\n",
        "kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /  tf.cast(x_train.shape[0], dtype=tf.float32))\n",
        "\n",
        "def prior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "    prior_model = tf.keras.Sequential(\n",
        "        [\n",
        "            tfp.layers.DistributionLambda(\n",
        "                lambda t: tfp.distributions.MultivariateNormalDiag(\n",
        "                    loc=tf.zeros(n), scale_diag=tf.ones(n)\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    return prior_model\n",
        "\n",
        "INPUT_SHAPE = (32,32,3)\n",
        "KERNEL_SIZE = (3,3)\n",
        "\n",
        "def posterior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "\n",
        "    # nvp = tfd.TransformedDistribution(\n",
        "    # distribution=tfd.MultivariateNormalDiag(n),\n",
        "    # bijector=tfb.RealNVP(\n",
        "    #     num_masked=2,\n",
        "    #     shift_and_log_scale_fn=tfb.real_nvp_default_template(\n",
        "    #         hidden_layers=[128, 128])))\n",
        "\n",
        "    # posterior_model = tf.keras.Sequential(\n",
        "    #     [\n",
        "    #         tfp.layers.VariableLayer(\n",
        "    #             tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype\n",
        "    #         ),\n",
        "    #         nvp,\n",
        "    #     ]\n",
        "    posterior_model = tfk.Sequential([\n",
        "    # NOTE: This model takes no input and outputs a Distribution.  (We use\n",
        "    # the batch_size and type of the input, but there are no actual input\n",
        "    # values because the last dimension of the shape is 0.)\n",
        "    #\n",
        "    # For conditional density estimation, the model would take the\n",
        "    # conditioning values as input.)\n",
        "    tfk.layers.InputLayer(input_shape=(0,), dtype=tf.float32),\n",
        "\n",
        "    # Given the empty input, return a standard normal distribution with\n",
        "    # matching batch_shape and event_shape of [2].\n",
        "    tfpl.DistributionLambda(lambda t: tfd.MultivariateNormalDiag(\n",
        "        # pylint: disable=g-long-lambda\n",
        "        loc=tf.zeros(tf.concat([tf.shape(t)[:-1], [2]], axis=0)),\n",
        "        scale_diag=[1., 1.])),\n",
        "\n",
        "    # Transform the standard normal distribution with event_shape of [2] to\n",
        "    # the target distribution with event_shape of [2].\n",
        "    tfpl.AutoregressiveTransform(tfb.AutoregressiveNetwork(\n",
        "        params=2, hidden_units=[10], activation='relu'))])\n",
        "    return posterior_model\n",
        "\n",
        "def get_conv_bayes(filters:int,activation=tf.nn.selu):\n",
        "\n",
        "    return tfp.layers.Convolution2DFlipout(filters=filters, kernel_size=KERNEL_SIZE, input_shape=INPUT_SHAPE,padding = \"same\",\n",
        "                                           bias_divergence_fn=kl_divergence_function,kernel_divergence_fn=kl_divergence_function)\n",
        "\n",
        "\n",
        "def get_dense_bayes(units:int,activation=tf.nn.selu):\n",
        "    return tfp.layers.DenseFlipout(units=units, activation=activation,bias_divergence_fn=kl_divergence_function,kernel_divergence_fn=kl_divergence_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VRN-C8G3QGS",
        "outputId": "f6d4a919-a97b-47fb-8253-a64cf62c0e00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:98: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
            "  loc = add_variable_fn(\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:108: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
            "  untransformed_scale = add_variable_fn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Convolutional Layer\n",
        "model.add(get_conv_bayes(32))\n",
        "model.add(BatchNormalization())\n",
        "model.add(get_conv_bayes(32))\n",
        "model.add(BatchNormalization())\n",
        "# Pooling layer\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "\n",
        "model.add(get_conv_bayes(64))\n",
        "model.add(BatchNormalization())\n",
        "model.add(get_conv_bayes(64))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(get_conv_bayes(128))\n",
        "model.add(BatchNormalization())\n",
        "model.add(get_conv_bayes(128))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(get_conv_bayes(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(get_conv_bayes(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "# model.add(get_conv_bayes(512))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(get_conv_bayes(512))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(get_dense_bayes(1024))\n",
        "model.add(BatchNormalization())\n",
        "# model.add(get_dense_bayes(256))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(get_dense_bayes(10, activation='softmax'))\n",
        "\n",
        "METRICS = [\n",
        "    'accuracy',\n",
        "    tf.keras.metrics.Precision(name='precision'),\n",
        "    tf.keras.metrics.Recall(name='recall')\n",
        "]\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=METRICS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh2T8nB6FPBm"
      },
      "outputs": [],
      "source": [
        "# INPUT_SHAPE = (32, 32, 3)\n",
        "# KERNEL_SIZE = (3, 3)\n",
        "# model = Sequential()\n",
        "\n",
        "# # Convolutional Layer\n",
        "# model.add(Conv2D(filters=32, kernel_size=KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(filters=32, kernel_size=KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
        "# model.add(BatchNormalization())\n",
        "# # Pooling layer\n",
        "# model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "# # Dropout layers\n",
        "# # model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "# model.add(Conv2D(filters=64, kernel_size=KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(filters=64, kernel_size=KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "# # model.add(Dropout(0.25))\n",
        "\n",
        "# model.add(Conv2D(filters=128, kernel_size=KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(filters=128, kernel_size=KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "# model.add(Conv2D(filters=256, kernel_size=KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(filters=256, kernel_size=KERNEL_SIZE, input_shape=INPUT_SHAPE, activation='relu', padding='same'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "# # model.add(Dropout(0.25))\n",
        "\n",
        "# model.add(Flatten())\n",
        "# # model.add(Dropout(0.2))\n",
        "# model.add(Dense(1024, activation='relu'))\n",
        "# # model.add(Dropout(0.25))\n",
        "# model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# METRICS = [\n",
        "#     'accuracy',\n",
        "#     tf.keras.metrics.Precision(name='precision'),\n",
        "#     tf.keras.metrics.Recall(name='recall')\n",
        "# ]\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=METRICS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3itcj3CIzaOu",
        "outputId": "868eb0f6-89f9-430e-9ebe-904b45f11af6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_flipout (Conv2DFlip  (None, 32, 32, 32)        1760      \n",
            " out)                                                            \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 32, 32, 32)        128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv2d_flipout_1 (Conv2DFl  (None, 32, 32, 32)        18464     \n",
            " ipout)                                                          \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 32, 32, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 16, 16, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_flipout_2 (Conv2DFl  (None, 16, 16, 64)        36928     \n",
            " ipout)                                                          \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 16, 16, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_flipout_3 (Conv2DFl  (None, 16, 16, 64)        73792     \n",
            " ipout)                                                          \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 16, 16, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_flipout_4 (Conv2DFl  (None, 8, 8, 128)         147584    \n",
            " ipout)                                                          \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 8, 8, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_flipout_5 (Conv2DFl  (None, 8, 8, 128)         295040    \n",
            " ipout)                                                          \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 8, 8, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_flipout_6 (Conv2DFl  (None, 4, 4, 256)         590080    \n",
            " ipout)                                                          \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 4, 4, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_flipout_7 (Conv2DFl  (None, 4, 4, 256)         1179904   \n",
            " ipout)                                                          \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 4, 4, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 2, 2, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_flipout (DenseFlipou  (None, 1024)              2098176   \n",
            " t)                                                              \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 1024)              4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_flipout_1 (DenseFlip  (None, 10)                20490     \n",
            " out)                                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4470154 (17.05 MB)\n",
            "Trainable params: 4466186 (17.04 MB)\n",
            "Non-trainable params: 3968 (15.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2mnlyTUteWi",
        "outputId": "de789981-149e-4c8e-fae7-961627f1af3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "1563/1563 [==============================] - 390s 244ms/step - loss: 112.9078 - accuracy: 0.2911 - precision: 0.3360 - recall: 0.2049 - val_loss: 108.5156 - val_accuracy: 0.3591 - val_precision: 0.4104 - val_recall: 0.2642\n",
            "Epoch 2/150\n",
            "1563/1563 [==============================] - 379s 242ms/step - loss: 102.2154 - accuracy: 0.4100 - precision: 0.4895 - recall: 0.3090 - val_loss: 95.4231 - val_accuracy: 0.4343 - val_precision: 0.5075 - val_recall: 0.3380\n",
            "Epoch 3/150\n",
            "1563/1563 [==============================] - 360s 230ms/step - loss: 87.5359 - accuracy: 0.4888 - precision: 0.5750 - recall: 0.3898 - val_loss: 79.8049 - val_accuracy: 0.5257 - val_precision: 0.6031 - val_recall: 0.4411\n",
            "Epoch 4/150\n",
            "1563/1563 [==============================] - 360s 231ms/step - loss: 72.3127 - accuracy: 0.5503 - precision: 0.6377 - recall: 0.4583 - val_loss: 65.2576 - val_accuracy: 0.5549 - val_precision: 0.6426 - val_recall: 0.4653\n",
            "Epoch 5/150\n",
            "1563/1563 [==============================] - 358s 229ms/step - loss: 58.7947 - accuracy: 0.5832 - precision: 0.6767 - recall: 0.4924 - val_loss: 52.8313 - val_accuracy: 0.5784 - val_precision: 0.6694 - val_recall: 0.4999\n",
            "Epoch 6/150\n",
            "1563/1563 [==============================] - 376s 241ms/step - loss: 47.3228 - accuracy: 0.6069 - precision: 0.7024 - recall: 0.5121 - val_loss: 42.3628 - val_accuracy: 0.6005 - val_precision: 0.6988 - val_recall: 0.5066\n",
            "Epoch 7/150\n",
            "1563/1563 [==============================] - 377s 241ms/step - loss: 37.7213 - accuracy: 0.6185 - precision: 0.7223 - recall: 0.5184 - val_loss: 33.5295 - val_accuracy: 0.6158 - val_precision: 0.7262 - val_recall: 0.5153\n",
            "Epoch 8/150\n",
            "1563/1563 [==============================] - 365s 233ms/step - loss: 29.6999 - accuracy: 0.6251 - precision: 0.7329 - recall: 0.5187 - val_loss: 26.2130 - val_accuracy: 0.6165 - val_precision: 0.7204 - val_recall: 0.5102\n",
            "Epoch 9/150\n",
            "1563/1563 [==============================] - 362s 231ms/step - loss: 23.0590 - accuracy: 0.6272 - precision: 0.7421 - recall: 0.5110 - val_loss: 20.2299 - val_accuracy: 0.6224 - val_precision: 0.7333 - val_recall: 0.5146\n",
            "Epoch 10/150\n",
            "1563/1563 [==============================] - 359s 229ms/step - loss: 17.8076 - accuracy: 0.6252 - precision: 0.7442 - recall: 0.5077 - val_loss: 15.6738 - val_accuracy: 0.6141 - val_precision: 0.7207 - val_recall: 0.5052\n",
            "Epoch 11/150\n",
            "1563/1563 [==============================] - 375s 240ms/step - loss: 13.8014 - accuracy: 0.6255 - precision: 0.7471 - recall: 0.5035 - val_loss: 12.1648 - val_accuracy: 0.6199 - val_precision: 0.7341 - val_recall: 0.5032\n",
            "Epoch 12/150\n",
            "1563/1563 [==============================] - 355s 227ms/step - loss: 10.8078 - accuracy: 0.6233 - precision: 0.7468 - recall: 0.4979 - val_loss: 9.5865 - val_accuracy: 0.6263 - val_precision: 0.7527 - val_recall: 0.5040\n",
            "Epoch 13/150\n",
            "1563/1563 [==============================] - 355s 227ms/step - loss: 8.6052 - accuracy: 0.6208 - precision: 0.7470 - recall: 0.4910 - val_loss: 7.7044 - val_accuracy: 0.6264 - val_precision: 0.7518 - val_recall: 0.5038\n",
            "Epoch 14/150\n",
            "1563/1563 [==============================] - 374s 240ms/step - loss: 6.9867 - accuracy: 0.6198 - precision: 0.7480 - recall: 0.4879 - val_loss: 6.3391 - val_accuracy: 0.6179 - val_precision: 0.7428 - val_recall: 0.4942\n",
            "Epoch 15/150\n",
            "1563/1563 [==============================] - 355s 227ms/step - loss: 5.8058 - accuracy: 0.6161 - precision: 0.7465 - recall: 0.4837 - val_loss: 5.3217 - val_accuracy: 0.6145 - val_precision: 0.7374 - val_recall: 0.4817\n",
            "Epoch 16/150\n",
            "1563/1563 [==============================] - 354s 226ms/step - loss: 4.9417 - accuracy: 0.6107 - precision: 0.7467 - recall: 0.4776 - val_loss: 4.5691 - val_accuracy: 0.6175 - val_precision: 0.7490 - val_recall: 0.4885\n",
            "Epoch 17/150\n",
            "1563/1563 [==============================] - 373s 238ms/step - loss: 4.3033 - accuracy: 0.6116 - precision: 0.7427 - recall: 0.4763 - val_loss: 4.0331 - val_accuracy: 0.6141 - val_precision: 0.7466 - val_recall: 0.4844\n",
            "Epoch 18/150\n",
            "1563/1563 [==============================] - 369s 236ms/step - loss: 3.8291 - accuracy: 0.6084 - precision: 0.7420 - recall: 0.4712 - val_loss: 3.6250 - val_accuracy: 0.6116 - val_precision: 0.7454 - val_recall: 0.4745\n",
            "Epoch 19/150\n",
            "1563/1563 [==============================] - 353s 226ms/step - loss: 3.4586 - accuracy: 0.6079 - precision: 0.7391 - recall: 0.4684 - val_loss: 3.2846 - val_accuracy: 0.6140 - val_precision: 0.7380 - val_recall: 0.4762\n",
            "Epoch 20/150\n",
            "1563/1563 [==============================] - 354s 226ms/step - loss: 3.1679 - accuracy: 0.6064 - precision: 0.7419 - recall: 0.4707 - val_loss: 3.0259 - val_accuracy: 0.6145 - val_precision: 0.7531 - val_recall: 0.4785\n",
            "Epoch 21/150\n",
            "1563/1563 [==============================] - 353s 226ms/step - loss: 2.9439 - accuracy: 0.6025 - precision: 0.7375 - recall: 0.4658 - val_loss: 2.8177 - val_accuracy: 0.6142 - val_precision: 0.7460 - val_recall: 0.4747\n",
            "Epoch 22/150\n",
            "1563/1563 [==============================] - 352s 225ms/step - loss: 2.7547 - accuracy: 0.6031 - precision: 0.7358 - recall: 0.4651 - val_loss: 2.6708 - val_accuracy: 0.6102 - val_precision: 0.7389 - val_recall: 0.4778\n",
            "Epoch 23/150\n",
            "1563/1563 [==============================] - 371s 238ms/step - loss: 2.6023 - accuracy: 0.5994 - precision: 0.7348 - recall: 0.4637 - val_loss: 2.5183 - val_accuracy: 0.6110 - val_precision: 0.7439 - val_recall: 0.4746\n",
            "Epoch 24/150\n",
            "1563/1563 [==============================] - 352s 225ms/step - loss: 2.4791 - accuracy: 0.6011 - precision: 0.7365 - recall: 0.4633 - val_loss: 2.3958 - val_accuracy: 0.6112 - val_precision: 0.7465 - val_recall: 0.4759\n",
            "Epoch 25/150\n",
            "1563/1563 [==============================] - 359s 230ms/step - loss: 2.3679 - accuracy: 0.6022 - precision: 0.7353 - recall: 0.4650 - val_loss: 2.3047 - val_accuracy: 0.6038 - val_precision: 0.7375 - val_recall: 0.4729\n",
            "Epoch 26/150\n",
            "1563/1563 [==============================] - 360s 230ms/step - loss: 2.2800 - accuracy: 0.6003 - precision: 0.7348 - recall: 0.4630 - val_loss: 2.2294 - val_accuracy: 0.6024 - val_precision: 0.7369 - val_recall: 0.4691\n",
            "Epoch 27/150\n",
            "1563/1563 [==============================] - 375s 240ms/step - loss: 2.2037 - accuracy: 0.5994 - precision: 0.7330 - recall: 0.4635 - val_loss: 2.1569 - val_accuracy: 0.6069 - val_precision: 0.7479 - val_recall: 0.4660\n",
            "Epoch 28/150\n",
            "1563/1563 [==============================] - 377s 241ms/step - loss: 2.1409 - accuracy: 0.5998 - precision: 0.7350 - recall: 0.4622 - val_loss: 2.0972 - val_accuracy: 0.6089 - val_precision: 0.7375 - val_recall: 0.4669\n",
            "Epoch 29/150\n",
            "1563/1563 [==============================] - 377s 241ms/step - loss: 2.0784 - accuracy: 0.5993 - precision: 0.7346 - recall: 0.4651 - val_loss: 2.0406 - val_accuracy: 0.6089 - val_precision: 0.7340 - val_recall: 0.4788\n",
            "Epoch 30/150\n",
            "1563/1563 [==============================] - 375s 240ms/step - loss: 2.0299 - accuracy: 0.6002 - precision: 0.7302 - recall: 0.4669 - val_loss: 1.9969 - val_accuracy: 0.6058 - val_precision: 0.7374 - val_recall: 0.4731\n",
            "Epoch 31/150\n",
            "1563/1563 [==============================] - 376s 240ms/step - loss: 1.9854 - accuracy: 0.6001 - precision: 0.7327 - recall: 0.4652 - val_loss: 1.9411 - val_accuracy: 0.6068 - val_precision: 0.7331 - val_recall: 0.4757\n",
            "Epoch 32/150\n",
            "1563/1563 [==============================] - 391s 250ms/step - loss: 1.9533 - accuracy: 0.5976 - precision: 0.7296 - recall: 0.4640 - val_loss: 1.9001 - val_accuracy: 0.6116 - val_precision: 0.7362 - val_recall: 0.4739\n",
            "Epoch 33/150\n",
            "1563/1563 [==============================] - 369s 236ms/step - loss: 1.9104 - accuracy: 0.5994 - precision: 0.7332 - recall: 0.4679 - val_loss: 1.8908 - val_accuracy: 0.6065 - val_precision: 0.7382 - val_recall: 0.4752\n",
            "Epoch 34/150\n",
            "1563/1563 [==============================] - 368s 235ms/step - loss: 1.8835 - accuracy: 0.6000 - precision: 0.7283 - recall: 0.4683 - val_loss: 1.8566 - val_accuracy: 0.6072 - val_precision: 0.7349 - val_recall: 0.4746\n",
            "Epoch 35/150\n",
            "1563/1563 [==============================] - 368s 235ms/step - loss: 1.8477 - accuracy: 0.6009 - precision: 0.7324 - recall: 0.4694 - val_loss: 1.8293 - val_accuracy: 0.6052 - val_precision: 0.7303 - val_recall: 0.4833\n",
            "Epoch 36/150\n",
            "1563/1563 [==============================] - 371s 237ms/step - loss: 1.8298 - accuracy: 0.6022 - precision: 0.7312 - recall: 0.4683 - val_loss: 1.8056 - val_accuracy: 0.6083 - val_precision: 0.7397 - val_recall: 0.4785\n",
            "Epoch 37/150\n",
            "1563/1563 [==============================] - 392s 251ms/step - loss: 1.8044 - accuracy: 0.6030 - precision: 0.7312 - recall: 0.4710 - val_loss: 1.7765 - val_accuracy: 0.6129 - val_precision: 0.7365 - val_recall: 0.4801\n",
            "Epoch 38/150\n",
            "1563/1563 [==============================] - 392s 251ms/step - loss: 1.7826 - accuracy: 0.6012 - precision: 0.7294 - recall: 0.4707 - val_loss: 1.7492 - val_accuracy: 0.6124 - val_precision: 0.7358 - val_recall: 0.4867\n",
            "Epoch 39/150\n",
            "1563/1563 [==============================] - 389s 249ms/step - loss: 1.7556 - accuracy: 0.6063 - precision: 0.7327 - recall: 0.4765 - val_loss: 1.7311 - val_accuracy: 0.6101 - val_precision: 0.7390 - val_recall: 0.4868\n",
            "Epoch 40/150\n",
            "1563/1563 [==============================] - 389s 249ms/step - loss: 1.7467 - accuracy: 0.6027 - precision: 0.7328 - recall: 0.4745 - val_loss: 1.7310 - val_accuracy: 0.6062 - val_precision: 0.7325 - val_recall: 0.4810\n",
            "Epoch 41/150\n",
            "1563/1563 [==============================] - 373s 239ms/step - loss: 1.7227 - accuracy: 0.6056 - precision: 0.7332 - recall: 0.4761 - val_loss: 1.6855 - val_accuracy: 0.6195 - val_precision: 0.7442 - val_recall: 0.4939\n",
            "Epoch 42/150\n",
            "1563/1563 [==============================] - 358s 229ms/step - loss: 1.7100 - accuracy: 0.6047 - precision: 0.7346 - recall: 0.4778 - val_loss: 1.6820 - val_accuracy: 0.6153 - val_precision: 0.7315 - val_recall: 0.4893\n",
            "Epoch 43/150\n",
            "1563/1563 [==============================] - 378s 242ms/step - loss: 1.6958 - accuracy: 0.6066 - precision: 0.7331 - recall: 0.4800 - val_loss: 1.6720 - val_accuracy: 0.6123 - val_precision: 0.7335 - val_recall: 0.4864\n",
            "Epoch 44/150\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 1.6832 - accuracy: 0.6048 - precision: 0.7315 - recall: 0.4778"
          ]
        }
      ],
      "source": [
        "model.fit(x_train, y_train_oh, epochs=150,validation_data=(x_test, y_test_oh))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYqxmqdcGTKT"
      },
      "outputs": [],
      "source": [
        "model.evaluate(x_test, y_test_oh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVlVmQPnGp2t"
      },
      "outputs": [],
      "source": [
        "lista = []\n",
        "for i in range(num_monte_carlo):\n",
        "  lista.append(model(x_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bW0OrA6nn_WS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "arr = np.array(lista)\n",
        "\n",
        "x = np.random.randint(0, x_test.shape[0])\n",
        "val = arr[:,x,:]\n",
        "img = x_test[x,:]\n",
        "\n",
        "label = y_test[x]\n",
        "#plt.figure(figsize=(10,10))\n",
        "print(f\"Label: {label}\")\n",
        "plt.imshow(img)\n",
        "val = pd.DataFrame(val)\n",
        "val.hist( bins=25,\n",
        "grid=False, figsize=(15,12), color='#86bf91', zorder=2, rwidth=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvmLKBmvGSNs"
      },
      "outputs": [],
      "source": [
        "## Texte Com Imagem False\n",
        "img_random = np.random.randint(0, 255, size=(32,32,3))/255.\n",
        "model.predict(img_random.reshape(1,32,32,3))\n",
        "plt.imshow(img_random)\n",
        "val = pd.DataFrame(val)\n",
        "val.hist( bins=25,\n",
        "grid=False, figsize=(15,12), color='#86bf91', zorder=2, rwidth=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_OAGF1JO-C1"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "img = Image.fromarray(x_train[999,:],\"RGB\")\n",
        "img.save(\"img.png\")\n",
        "img.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOD1PXDEAh7Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNQepFmIBI9bxISCxFS4K3F",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}